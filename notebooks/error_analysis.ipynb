{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Error Analysis\n",
                "Analysis of model errors, focusing on \"reasonably wrong\" predictions (high confidence misclassifications)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torchvision import transforms\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import sys\n",
                "import torch.nn.functional as F\n",
                "\n",
                "sys.path.append('..')\n",
                "from src.models.classifier import UrbanIssuesClassifier\n",
                "from src.data.dataset import UrbanIssuesDataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Model and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Classes: ['Damaged concrete structures', 'DamagedElectricalPoles', 'DamagedRoadSigns', 'DeadAnimalsPollution', 'FallenTrees', 'Garbage', 'Graffitti', 'IllegalParking', 'Potholes and RoadCracks']\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "num_classes = 9\n",
                "model = UrbanIssuesClassifier(num_classes=num_classes, pretrained=False)\n",
                "model.load_state_dict(torch.load('../best_model.pth', map_location=device))\n",
                "model.to(device)\n",
                "model.eval()\n",
                "\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "])\n",
                "\n",
                "# We need the original images for visualization, so we'll use a separate dataset without normalization for plotting\n",
                "viz_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "])\n",
                "\n",
                "valid_dataset = UrbanIssuesDataset(root_dir='../data', split='valid', transform=transform)\n",
                "viz_dataset = UrbanIssuesDataset(root_dir='../data', split='valid', transform=viz_transform)\n",
                "\n",
                "classes = valid_dataset.classes\n",
                "print(f\"Classes: {classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Identify Errors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scanning for errors...\n",
                        "Found 193 errors.\n"
                    ]
                }
            ],
            "source": [
                "errors = []\n",
                "\n",
                "loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
                "\n",
                "print(\"Scanning for errors...\")\n",
                "with torch.no_grad():\n",
                "    for i, (image, label) in enumerate(loader):\n",
                "        image = image.to(device)\n",
                "        output = model(image)\n",
                "        probs = F.softmax(output, dim=1)\n",
                "        confidence, predicted = torch.max(probs, 1)\n",
                "        \n",
                "        if predicted.item() != label.item():\n",
                "            errors.append({\n",
                "                'index': i,\n",
                "                'true_label': classes[label.item()],\n",
                "                'pred_label': classes[predicted.item()],\n",
                "                'confidence': confidence.item(),\n",
                "                'probs': probs.cpu().numpy()[0]\n",
                "            })\n",
                "\n",
                "# Sort by confidence (descending) to find \"reasonably wrong\" errors\n",
                "errors.sort(key=lambda x: x['confidence'], reverse=True)\n",
                "print(f\"Found {len(errors)} errors.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize Top Errors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_errors(errors, n=10):\n",
                "    count = min(len(errors), n)\n",
                "    cols = 2\n",
                "    rows = (count + 1) // 2\n",
                "    \n",
                "    plt.figure(figsize=(15, 5 * rows))\n",
                "    \n",
                "    for i in range(count):\n",
                "        error = errors[i]\n",
                "        idx = error['index']\n",
                "        \n",
                "        # Get original image for visualization\n",
                "        img, _ = viz_dataset[idx]\n",
                "        img = img.permute(1, 2, 0).numpy()\n",
                "        \n",
                "        plt.subplot(rows, cols, i + 1)\n",
                "        plt.imshow(img)\n",
                "        plt.title(f\"True: {error['true_label']}\\nPred: {error['pred_label']} ({error['confidence']:.2f})\", \n",
                "                  color='red', fontsize=12, fontweight='bold')\n",
                "        plt.axis('off')\n",
                "        \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# show_errors(errors, n=30)\n",
                "filtered_errors = [error for error in errors if error['true_label'] == 'Potholes and RoadCracks' and error['pred_label'] == 'Garbage']\n",
                "filtered_errors2 = [error for error in errors if error['pred_label'] == 'Potholes and RoadCracks' and error['true_label'] == 'Garbage']\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "aac4586d",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 1500x0 with 0 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "show_errors(filtered_errors, n=30)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "urban_issues_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
